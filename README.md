# transformer

![transformer](dowloads/attention_model-precise.png?raw=true "transformer")

*attention is all you need* - somebody more important than me

transformer with autocomplete feature. trained with kaggle-amazon review set

## todo

 - [x] create working transformer
 - [x] create working encoder
 - [x] position encoder
 - [x] multiheadattention
 - [x] masked multiheadattention
 - [x] create working decoder
 - [ ] handle text data
 - [ ] handle batches

## useful links

 * [nlp from scratch by pytorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
 * [gru](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)
 * [multiheadattention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)
 * [embeddings](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
 * [transformer from scratch](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)
 * [positional encoder](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
 * [what is positional encoding](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model)
 * [visual guide to positional encoding](https://www.youtube.com/watch?v=dichIcUZfOw)
 * [master position encoding part 1](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)
 * [master position encoding part 2](https://towardsdatascience.com/master-positional-encoding-part-ii-1cfc4d3e7375)
 * [attention is all you need](https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/fec78a687210851f055f792d45300d27cc60ae41)
 * [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)
 * [torch multi head attention](https://github.com/CyberZHG/torch-multi-head-attention)